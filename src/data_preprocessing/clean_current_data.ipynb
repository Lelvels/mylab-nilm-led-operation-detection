{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from data_repository import DataRepository\n",
    "from confident_learning import MyConfidentLearning\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repo = DataRepository(\"../.env\")\n",
    "# Load unclean current data\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = data_repo.load_current_data(clean_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names, validation_file_names, test_file_names = data_repo.load_unclean_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8649, 9000) (8649,) (1191, 9000) (1191,)\n",
      "8649 1191\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_validation.shape, y_validation.shape)\n",
    "print(len(train_file_names), len(validation_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 1718,\n",
       " 'normal': 1734,\n",
       " 'overcurrent': 1709,\n",
       " 'overheating': 1766,\n",
       " 'zero': 1722}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_repo.count_labels(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Label encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi danh sách labels thành mã số\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_validation = label_encoder.transform(y_validation)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['error', 'normal', 'overcurrent', 'overheating', 'zero']\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(list(label_encoder.classes_))\n",
    "print(label_encoder.transform(list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we have to check the model's accuracy on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model's underfit, the model won't fit the data, therefore the out-of-sample probality won't describe correctly the data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True)\n",
    "# train_acc = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5, verbose=10)\n",
    "# validation_acc = cross_val_score(model, X_validation, y_validation, scoring='accuracy', cv=5, verbose=10)\n",
    "# test_acc = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.average(train_acc), np.average(test_acc), np.average(validation_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the accuracy of the cross validation are reasonable, confident learning can be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting out of sample predicted probality__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Getting out of sample probality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Finished getting out of sample probality with shape: (8649, 5)\n",
      "[+] Getting out of sample probality\n",
      "[-] Finished getting out of sample probality with shape: (1191, 5)\n",
      "[+] Getting out of sample probality\n",
      "[-] Finished getting out of sample probality with shape: (2160, 5)\n"
     ]
    }
   ],
   "source": [
    "train_cl = MyConfidentLearning(X=X_train, y=y_train)\n",
    "train_pred_probs = train_cl.get_out_of_sample_proba()\n",
    "\n",
    "validation_cl = MyConfidentLearning(X=X_validation, y=y_validation)\n",
    "validation_pred_probs = validation_cl.get_out_of_sample_proba()\n",
    "\n",
    "test_cl = MyConfidentLearning(X=X_test, y=y_test)\n",
    "test_pred_probs = test_cl.get_out_of_sample_proba()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing class thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.8616693  0.91322388 0.95346248 0.89964409 0.90142361]\n",
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.91641633 0.90305904 0.92666287 0.83581653 0.91424131]\n",
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.84152774 0.92196723 0.92664007 0.82998395 0.90748653]\n"
     ]
    }
   ],
   "source": [
    "# should be a numpy array of length 5\n",
    "train_thresholds = train_cl.compute_class_thresholds()\n",
    "validation_thresholds = validation_cl.compute_class_thresholds()\n",
    "test_thresholds = test_cl.compute_class_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the confident joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0   33   63]\n",
      " [   4 1587   26   92    0]\n",
      " [   0   56 1615    0    0]\n",
      " [  45   33    0 1467    0]\n",
      " [ 109    0    0    0 1483]]\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   2   4]\n",
      " [  0 223   0  18   0]\n",
      " [  0  17 220   0   0]\n",
      " [  7  16   0 173   0]\n",
      " [  4   0   0   0 212]]\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0  16  15]\n",
      " [  0 377   7  18   0]\n",
      " [  0  33 421   0   0]\n",
      " [ 16  21   0 305   0]\n",
      " [ 22   0   0   0 374]]\n"
     ]
    }
   ],
   "source": [
    "C_train = train_cl.compute_confident_joint()\n",
    "C_validation = validation_cl.compute_confident_joint()\n",
    "C_test = test_cl.compute_confident_joint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 - Methods 2: Estimate the labels errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_noise_rate(C, no_of_samples):\n",
    "    num_label_issues = np.sum(C - np.diag(np.diag(C)))\n",
    "    print(f\"Number of label issues: {num_label_issues}\")\n",
    "    print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / no_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 461\n",
      "Estimated noise rate: 5.3%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_train, train_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 148\n",
      "Estimated noise rate: 6.9%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_test, test_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 68\n",
      "Estimated noise rate: 5.7%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_validation, validation_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning label issues\n",
    "\n",
    "First indentify the label issues, which are the off-diagonal elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finding labels issue indeces:\n",
      "Issue indices: 461\n",
      "[+] Finding labels issue indeces:\n",
      "Issue indices: 68\n",
      "[+] Finding labels issue indeces:\n",
      "Issue indices: 148\n"
     ]
    }
   ],
   "source": [
    "train_issue_indices = train_cl.find_label_issues()\n",
    "validation_issue_indices = validation_cl.find_label_issues()\n",
    "test_issue_indices = test_cl.find_label_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check the confident joint again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the label errors found by Confident Learning\n",
    "clean_X_train = np.delete(X_train, train_issue_indices, axis=0) \n",
    "clean_y_train = np.delete(y_train, train_issue_indices)\n",
    "clean_train_file_names = np.delete(train_file_names, train_issue_indices)\n",
    "clean_train_pred_probs = np.delete(train_cl.pred_probs, train_issue_indices, axis=0)\n",
    "\n",
    "clean_X_validation = np.delete(X_validation, validation_issue_indices, axis=0) \n",
    "clean_y_validation = np.delete(y_validation, validation_issue_indices)\n",
    "clean_validation_file_names = np.delete(validation_file_names, validation_issue_indices)\n",
    "clean_validation_pred_probs = np.delete(validation_cl.pred_probs, validation_issue_indices, axis=0)\n",
    "\n",
    "clean_X_test = np.delete(X_test, test_issue_indices, axis=0) \n",
    "clean_y_test = np.delete(y_test, test_issue_indices)\n",
    "clean_test_file_names = np.delete(test_file_names, test_issue_indices)\n",
    "clean_test_pred_probs = np.delete(test_cl.pred_probs, test_issue_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint(pred_probs: np.ndarray, thresholds: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    print(\"[+] Computing confident joint\")\n",
    "    n_examples, n_classes = pred_probs.shape\n",
    "    confident_joint = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    for data_idx in range(n_examples):\n",
    "        i = labels[data_idx]    #y_noise\n",
    "        j = None                #y_true -> to find\n",
    "        #Lưu ý điểm mình bị sai: vị trí của chúng không ứng với label\n",
    "        p_j = -1\n",
    "        for candidate_j in range(n_classes):\n",
    "            p = pred_probs[data_idx, candidate_j]\n",
    "            if p >= thresholds[candidate_j] and p > p_j:\n",
    "                j = candidate_j\n",
    "                p_j = p\n",
    "        if j is not None:\n",
    "            confident_joint[i][j] += 1\n",
    "    print(\"[-] Finished compute confident joint:\")\n",
    "    print(confident_joint)\n",
    "    return confident_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check the confident joint again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0    0    0]\n",
      " [   0 1587    0    0    0]\n",
      " [   0    0 1615    0    0]\n",
      " [   0    0    0 1467    0]\n",
      " [   0    0    0    0 1483]]\n",
      "Validation dataset\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   0   0]\n",
      " [  0 223   0   0   0]\n",
      " [  0   0 220   0   0]\n",
      " [  0   0   0 173   0]\n",
      " [  0   0   0   0 212]]\n",
      "Test dataset\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0   0   0]\n",
      " [  0 377   0   0   0]\n",
      " [  0   0 421   0   0]\n",
      " [  0   0   0 305   0]\n",
      " [  0   0   0   0 374]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset:\")\n",
    "clean_C_train = compute_confident_joint(pred_probs=clean_train_pred_probs, thresholds=train_thresholds, labels=clean_y_train)\n",
    "\n",
    "print(\"Validation dataset\")\n",
    "clean_C_validation = compute_confident_joint(pred_probs=clean_validation_pred_probs, thresholds=validation_thresholds, labels=clean_y_validation)\n",
    "\n",
    "print(\"Test dataset\")\n",
    "clean_C_test = compute_confident_joint(pred_probs=clean_test_pred_probs, thresholds=test_thresholds, labels=clean_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The results: all the off-diagonal elements are removed!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if their subtraction are corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8188, 9000) 8188 8188 (8188, 5)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0   33   63]\n",
      " [   4 1587   26   92    0]\n",
      " [   0   56 1615    0    0]\n",
      " [  45   33    0 1467    0]\n",
      " [ 109    0    0    0 1483]]\n",
      "{0: 1718, 1: 1734, 2: 1709, 3: 1766, 4: 1722}\n",
      "{0: 1622, 1: 1612, 2: 1653, 3: 1688, 4: 1613}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_train.shape, len(clean_y_train), len(clean_train_file_names), clean_train_pred_probs.shape)\n",
    "train_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(train_cl.y))\n",
    "print(data_repo.count_labels(clean_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1123, 9000) 1123 1123 (1123, 5)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   2   4]\n",
      " [  0 223   0  18   0]\n",
      " [  0  17 220   0   0]\n",
      " [  7  16   0 173   0]\n",
      " [  4   0   0   0 212]]\n",
      "{0: 237, 1: 247, 2: 237, 3: 223, 4: 247}\n",
      "{0: 231, 1: 229, 2: 220, 3: 200, 4: 243}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_validation.shape, len(clean_y_validation), len(clean_validation_file_names), clean_validation_pred_probs.shape)\n",
    "validation_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(validation_cl.y))\n",
    "print(data_repo.count_labels(clean_y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2012, 9000) 2012 2012 (2012,)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0  16  15]\n",
      " [  0 377   7  18   0]\n",
      " [  0  33 421   0   0]\n",
      " [ 16  21   0 305   0]\n",
      " [ 22   0   0   0 374]]\n",
      "{0: 445, 1: 419, 2: 454, 3: 411, 4: 431}\n",
      "{0: 414, 1: 394, 2: 421, 3: 374, 4: 409}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_test.shape, len(clean_X_test), len(clean_X_test), clean_test_file_names.shape)\n",
    "test_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(test_cl.y))\n",
    "print(data_repo.count_labels(clean_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models with different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True)\n",
    "# # Train model on original, possibly noisy data.\n",
    "# model.fit(train_cl.X, train_cl.y)\n",
    "# # Evaluate model on test split with ground truth labels.\n",
    "# preds = model.predict(X_validation)\n",
    "# acc_original = accuracy_score(preds, y_validation)\n",
    "# print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_model = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True)\n",
    "# # Train model on original, possibly noisy data.\n",
    "# clean_model.fit(clean_X_train, clean_y_train)\n",
    "# # Evaluate model on test split with ground truth labels.\n",
    "# clean_preds = clean_model.predict(clean_X_validation)\n",
    "# new_acc = accuracy_score(clean_preds, clean_y_validation)\n",
    "# print(f\"Accuracy with clean data: {round(new_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model on test split with ground truth labels.\n",
    "# preds = model.predict(clean_X_validation)\n",
    "# test_acc = accuracy_score(clean_preds, clean_y_validation)\n",
    "# print(f\"Accuracy with original data: {round(test_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for survey Confident Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = np.array(train_file_names)\n",
    "test_file_names = np.array(test_file_names)\n",
    "validation_file_names = np.array(validation_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_y_train = y_train[train_issue_indices]\n",
    "dirty_y_val = y_validation[validation_issue_indices]\n",
    "dirty_y_test =y_test[test_issue_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 68 148\n"
     ]
    }
   ],
   "source": [
    "dirty_train_files = train_file_names[train_issue_indices]\n",
    "dirty_validation_files = validation_file_names[validation_issue_indices]\n",
    "dirty_test_files = test_file_names[test_issue_indices]\n",
    "print(len(dirty_train_files), len(dirty_validation_files), len(dirty_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8649,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files that contain the text 'error' in their file name is 96.\n",
      "The number of files that contain the text 'normal' in their file name is 122.\n",
      "The number of files that contain the text 'overcurrent' in their file name is 56.\n",
      "The number of files that contain the text 'overheating' in their file name is 78.\n",
      "The number of files that contain the text 'zero' in their file name is 109.\n"
     ]
    }
   ],
   "source": [
    "search_terms = [\"error\", \"normal\", \"overcurrent\", \"overheating\", \"zero\"]\n",
    "count = {term: 0 for term in search_terms}\n",
    "\n",
    "for filename in dirty_train_files:\n",
    "    for term in search_terms:\n",
    "        if term in filename:\n",
    "            count[term] += 1\n",
    "\n",
    "for term, value in count.items():\n",
    "    print(f\"The number of files that contain the text '{term}' in their file name is {value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 96, 1: 122, 2: 56, 3: 78, 4: 109}\n",
      "{0: 31, 1: 25, 2: 33, 3: 37, 4: 22}\n"
     ]
    }
   ],
   "source": [
    "print(data_repo.count_labels(dirty_y_train))\n",
    "print(data_repo.count_labels(dirty_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(X, y, test_size, random_state):\n",
    "    # Create a dictionary to store the indices of each class\n",
    "    class_indices = {}\n",
    "    for i, label in enumerate(y):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(i)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    num_samples = {label: int(test_size * len(indices)) for label, indices in class_indices.items()}\n",
    "\n",
    "    # Set the random seed\n",
    "    random.seed(random_state)\n",
    "\n",
    "    # Randomly select samples from each class\n",
    "    test_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        test_indices += random.sample(indices, num_samples[label])\n",
    "\n",
    "    # Create the training and test sets\n",
    "    X_train = [X[i] for i in range(len(X)) if i not in test_indices]\n",
    "    y_train = [y[i] for i in range(len(y)) if i not in test_indices]\n",
    "    X_test = [X[i] for i in range(len(X)) if i in test_indices]\n",
    "    y_test = [y[i] for i in range(len(y)) if i in test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Working with dirty size: 0.1 on dataset 0!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1036.csv' 'error_data2552.csv' 'error_data2558.csv'\n",
      " 'error_data2590.csv' 'error_data2592.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 1!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1047.csv' 'error_data1074.csv' 'error_data1083.csv'\n",
      " 'error_data2550.csv' 'error_data2608.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 2!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1046.csv' 'error_data1057.csv' 'error_data1062.csv'\n",
      " 'error_data1098.csv' 'error_data2550.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 3!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1080.csv' 'error_data2544.csv' 'error_data2569.csv'\n",
      " 'error_data2619.csv' 'error_data281.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 4!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1047.csv' 'error_data1062.csv' 'error_data1072.csv'\n",
      " 'error_data1091.csv' 'error_data2544.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 5!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1017.csv' 'error_data2550.csv' 'error_data2566.csv'\n",
      " 'error_data2615.csv' 'error_data278.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 6!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1023.csv' 'error_data1057.csv'\n",
      " 'error_data1085.csv' 'error_data2552.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 7!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1038.csv' 'error_data1055.csv' 'error_data1067.csv'\n",
      " 'error_data1091.csv' 'error_data2561.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 8!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1036.csv' 'error_data1057.csv' 'error_data1080.csv'\n",
      " 'error_data1083.csv' 'error_data2518.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.1 on dataset 9!\n",
      "[-] Number of labels removed: X-43, y-43, follow-rate-46.1\n",
      "[-] Removed labels distribution: {0: 9, 1: 12, 2: 5, 3: 7, 4: 10}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1083.csv' 'error_data2513.csv'\n",
      " 'error_data2553.csv' 'error_data2563.csv']\n",
      "[-] Check number of files: 8606\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 0!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1036.csv' 'error_data1067.csv' 'error_data1083.csv'\n",
      " 'error_data2529.csv' 'error_data2550.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 1!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1017.csv' 'error_data1047.csv'\n",
      " 'error_data1067.csv' 'error_data1074.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 2!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1023.csv' 'error_data1046.csv' 'error_data1057.csv'\n",
      " 'error_data1062.csv' 'error_data1097.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 3!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1047.csv' 'error_data1080.csv'\n",
      " 'error_data1091.csv' 'error_data2518.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 4!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1046.csv' 'error_data1047.csv'\n",
      " 'error_data1062.csv' 'error_data1072.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 5!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1017.csv' 'error_data1038.csv' 'error_data1072.csv'\n",
      " 'error_data1073.csv' 'error_data1097.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 6!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1013.csv' 'error_data1023.csv'\n",
      " 'error_data1057.csv' 'error_data1085.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 7!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1023.csv' 'error_data1038.csv' 'error_data1046.csv'\n",
      " 'error_data1047.csv' 'error_data1055.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 8!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1017.csv' 'error_data1036.csv' 'error_data1057.csv'\n",
      " 'error_data1080.csv' 'error_data1083.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.2 on dataset 9!\n",
      "[-] Number of labels removed: X-90, y-90, follow-rate-92.2\n",
      "[-] Removed labels distribution: {0: 19, 1: 24, 2: 11, 3: 15, 4: 21}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1036.csv' 'error_data1057.csv'\n",
      " 'error_data1083.csv' 'error_data1098.csv']\n",
      "[-] Check number of files: 8559\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 0!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1017.csv' 'error_data1036.csv' 'error_data1055.csv'\n",
      " 'error_data1067.csv' 'error_data1083.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 1!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1047.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 2!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1046.csv' 'error_data1047.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 3!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1017.csv' 'error_data1036.csv'\n",
      " 'error_data1047.csv' 'error_data1080.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 4!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1017.csv' 'error_data1046.csv'\n",
      " 'error_data1047.csv' 'error_data1062.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 5!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1017.csv'\n",
      " 'error_data1038.csv' 'error_data1055.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 6!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1013.csv' 'error_data1023.csv'\n",
      " 'error_data1057.csv' 'error_data1062.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 7!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1023.csv' 'error_data1036.csv' 'error_data1038.csv'\n",
      " 'error_data1046.csv' 'error_data1047.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 8!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.4 on dataset 9!\n",
      "[-] Number of labels removed: X-182, y-182, follow-rate-184.4\n",
      "[-] Removed labels distribution: {0: 38, 1: 48, 2: 22, 3: 31, 4: 43}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1036.csv' 'error_data1038.csv'\n",
      " 'error_data1047.csv' 'error_data1057.csv']\n",
      "[-] Check number of files: 8467\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 0!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1055.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 1!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 2!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1046.csv' 'error_data1047.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 3!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1013.csv' 'error_data1017.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 4!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1013.csv' 'error_data1017.csv'\n",
      " 'error_data1036.csv' 'error_data1046.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 5!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1017.csv'\n",
      " 'error_data1038.csv' 'error_data1055.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 6!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1023.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 7!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1017.csv' 'error_data1023.csv' 'error_data1036.csv'\n",
      " 'error_data1038.csv' 'error_data1046.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 8!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.6 on dataset 9!\n",
      "[-] Number of labels removed: X-274, y-274, follow-rate-276.59999999999997\n",
      "[-] Removed labels distribution: {0: 57, 1: 73, 2: 33, 3: 46, 4: 65}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1036.csv']\n",
      "[-] Check number of files: 8375\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 0!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 1!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1036.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 2!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1008.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1046.csv' 'error_data1047.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 3!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1036.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 4!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1013.csv' 'error_data1017.csv'\n",
      " 'error_data1023.csv' 'error_data1036.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 5!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1023.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 6!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1023.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 7!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 8!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1013.csv' 'error_data1017.csv' 'error_data1023.csv'\n",
      " 'error_data1036.csv' 'error_data1038.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n",
      "[+] Working with dirty size: 0.8 on dataset 9!\n",
      "[-] Number of labels removed: X-366, y-366, follow-rate-368.8\n",
      "[-] Removed labels distribution: {0: 76, 1: 97, 2: 44, 3: 62, 4: 87}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1008.csv' 'error_data1013.csv'\n",
      " 'error_data1017.csv' 'error_data1023.csv']\n",
      "[-] Check number of files: 8283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dirty_size in [0.1, 0.2, 0.4, 0.6, 0.8]:\n",
    "    fp_train_dfs = []\n",
    "    for i in range(0, 10):\n",
    "        print(f\"[+] Working with dirty size: {dirty_size} on dataset {i}!\")\n",
    "        _, fp_dirty_train, _, y_removed_labels = split_dataset(dirty_train_files, dirty_y_train, \n",
    "                                                        test_size=dirty_size, random_state=i)\n",
    "        print(f\"[-] Number of labels removed: X-{len(fp_dirty_train)}, y-{len(y_removed_labels)}, follow-rate-{dirty_size*len(dirty_train_files)}\")\n",
    "        print(f\"[-] Removed labels distribution: {data_repo.count_labels(y_removed_labels)}\")\n",
    "        current_dirty_indeces = []\n",
    "        for dirty_file in fp_dirty_train:\n",
    "            current_dirty_indeces.append(np.where(train_file_names == dirty_file)[0][0])\n",
    "        current_dirty_indeces = np.array(current_dirty_indeces)\n",
    "        print(f\"[-] Check randomness: {train_file_names[current_dirty_indeces[0:5]]}\")\n",
    "        clean_file_names = np.delete(train_file_names, current_dirty_indeces)\n",
    "        print(f\"[-] Check number of files: {len(clean_file_names)}\")\n",
    "        print()\n",
    "        clean_file_df = pd.DataFrame({\"files\": clean_file_names})\n",
    "        fp_train_dfs.append(clean_file_df)\n",
    "    file_name = f\"../../data/cl_survey_data/train_dataset/cl_train_files_{dirty_size}_dirty_removed.xlsx\"\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        i = 0\n",
    "        for fp_train_df in fp_train_dfs:\n",
    "            sheet_name = \"Train_dataset_\"+str(i)\n",
    "            fp_train_df.to_excel(writer, sheet_name=sheet_name,index=False)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Working with dirty size: 0.1 on test dataset 0!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1032.csv' 'error_data1412.csv' 'error_data1424.csv'\n",
      " 'error_data1672.csv' 'error_data1710.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 1!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1012.csv' 'error_data1354.csv' 'error_data1424.csv'\n",
      " 'error_data1673.csv' 'error_data1780.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 2!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1424.csv' 'error_data1435.csv'\n",
      " 'error_data1673.csv' 'error_data1709.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 3!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1019.csv' 'error_data1352.csv' 'error_data1354.csv'\n",
      " 'error_data1677.csv' 'error_data1709.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 4!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data101.csv' 'error_data1019.csv' 'error_data1022.csv'\n",
      " 'error_data171.csv' 'error_data1779.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 5!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1021.csv' 'error_data1361.csv' 'error_data1411.csv'\n",
      " 'error_data1709.csv' 'error_data1776.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 6!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1354.csv' 'error_data1414.csv' 'error_data1423.csv'\n",
      " 'error_data1673.csv' 'error_data1712.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 7!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1012.csv' 'error_data1023.csv' 'error_data1435.csv'\n",
      " 'error_data171.csv' 'error_data1774.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 8!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1019.csv' 'error_data1026.csv' 'error_data1435.csv'\n",
      " 'error_data1677.csv' 'error_data171.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.1 on test dataset 9!\n",
      "[-] Number of labels removed: X-13, y-13, follow-rate-14.8\n",
      "[-] Removed labels distribution: {0: 3, 1: 2, 2: 3, 3: 3, 4: 2}\n",
      "[-] Check randomness: ['error_data1026.csv' 'error_data1035.csv' 'error_data1361.csv'\n",
      " 'error_data1677.csv' 'error_data1706.csv']\n",
      "[-] Check number of files: 2147\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 0!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1021.csv' 'error_data1032.csv'\n",
      " 'error_data1033.csv' 'error_data1412.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 1!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1012.csv' 'error_data1354.csv'\n",
      " 'error_data1412.csv' 'error_data1414.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 2!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1423.csv'\n",
      " 'error_data1424.csv' 'error_data1434.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 3!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1012.csv' 'error_data1019.csv' 'error_data1026.csv'\n",
      " 'error_data1352.csv' 'error_data1354.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 4!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data101.csv' 'error_data1019.csv' 'error_data1022.csv'\n",
      " 'error_data1032.csv' 'error_data1333.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 5!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1021.csv' 'error_data1026.csv' 'error_data1361.csv'\n",
      " 'error_data1393.csv' 'error_data1411.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 6!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data1333.csv' 'error_data1354.csv'\n",
      " 'error_data1412.csv' 'error_data1414.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 7!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1023.csv' 'error_data1032.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 8!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1012.csv' 'error_data1016.csv' 'error_data1019.csv'\n",
      " 'error_data1026.csv' 'error_data1032.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.2 on test dataset 9!\n",
      "[-] Number of labels removed: X-28, y-28, follow-rate-29.6\n",
      "[-] Removed labels distribution: {0: 6, 1: 5, 2: 6, 3: 7, 4: 4}\n",
      "[-] Check randomness: ['error_data1012.csv' 'error_data1015.csv' 'error_data1021.csv'\n",
      " 'error_data1026.csv' 'error_data1035.csv']\n",
      "[-] Check number of files: 2132\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 0!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1021.csv' 'error_data1022.csv'\n",
      " 'error_data1026.csv' 'error_data1032.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 1!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1007.csv' 'error_data101.csv' 'error_data1012.csv'\n",
      " 'error_data1021.csv' 'error_data1032.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 2!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1015.csv'\n",
      " 'error_data1021.csv' 'error_data1022.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 3!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1019.csv' 'error_data1026.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 4!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 5!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1019.csv' 'error_data1021.csv'\n",
      " 'error_data1026.csv' 'error_data1035.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 6!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data1012.csv' 'error_data1021.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 7!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1023.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 8!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1016.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.4 on test dataset 9!\n",
      "[-] Number of labels removed: X-57, y-57, follow-rate-59.2\n",
      "[-] Removed labels distribution: {0: 12, 1: 10, 2: 13, 3: 14, 4: 8}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1012.csv' 'error_data1015.csv'\n",
      " 'error_data1021.csv' 'error_data1023.csv']\n",
      "[-] Check number of files: 2103\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 0!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1016.csv' 'error_data1021.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 1!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 2!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1015.csv'\n",
      " 'error_data1016.csv' 'error_data1021.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 3!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1016.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 4!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 5!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data101.csv'\n",
      " 'error_data1015.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 6!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data1012.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 7!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 8!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data1012.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.6 on test dataset 9!\n",
      "[-] Number of labels removed: X-87, y-87, follow-rate-88.8\n",
      "[-] Removed labels distribution: {0: 18, 1: 15, 2: 19, 3: 22, 4: 13}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data1012.csv' 'error_data1015.csv']\n",
      "[-] Check number of files: 2073\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 0!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1012.csv'\n",
      " 'error_data1016.csv' 'error_data1021.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 1!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 2!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1006.csv' 'error_data1007.csv' 'error_data1015.csv'\n",
      " 'error_data1016.csv' 'error_data1019.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 3!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1007.csv' 'error_data101.csv'\n",
      " 'error_data1012.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 4!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 5!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data101.csv'\n",
      " 'error_data1015.csv' 'error_data1016.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 6!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 7!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 8!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n",
      "[+] Working with dirty size: 0.8 on test dataset 9!\n",
      "[-] Number of labels removed: X-116, y-116, follow-rate-118.4\n",
      "[-] Removed labels distribution: {0: 24, 1: 20, 2: 26, 3: 29, 4: 17}\n",
      "[-] Check randomness: ['error_data1005.csv' 'error_data1006.csv' 'error_data1007.csv'\n",
      " 'error_data101.csv' 'error_data1012.csv']\n",
      "[-] Check number of files: 2044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dirty_size in [0.1, 0.2, 0.4, 0.6, 0.8]:\n",
    "    fp_test_dfs = []\n",
    "    for i in range(0, 10):\n",
    "        print(f\"[+] Working with dirty size: {dirty_size} on test dataset {i}!\")\n",
    "        _, fp_dirty, _, y_removed_labels = split_dataset(dirty_test_files, dirty_y_test, \n",
    "                                                        test_size=dirty_size, random_state=i)\n",
    "        print(f\"[-] Number of labels removed: X-{len(fp_dirty)}, y-{len(y_removed_labels)}, follow-rate-{dirty_size*len(dirty_test_files)}\")\n",
    "        print(f\"[-] Removed labels distribution: {data_repo.count_labels(y_removed_labels)}\")\n",
    "        current_dirty_indeces = []\n",
    "        for dirty_file in fp_dirty:\n",
    "            current_dirty_indeces.append(np.where(test_file_names == dirty_file)[0][0])\n",
    "        current_dirty_indeces = np.array(current_dirty_indeces)\n",
    "        clean_file_names = np.delete(test_file_names, current_dirty_indeces)\n",
    "        print(f\"[-] Check randomness: {train_file_names[current_dirty_indeces[0:5]]}\")\n",
    "        print(f\"[-] Check number of files: {len(clean_file_names)}\")\n",
    "        clean_file_df = pd.DataFrame({\"files\": clean_file_names})\n",
    "        fp_test_dfs.append(clean_file_df)\n",
    "        print()\n",
    "    file_name = f\"../../data/cl_survey_data/test_dataset/cl_test_files_{dirty_size}_dirty_removed.xlsx\"\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        i = 0\n",
    "        for fp_test_df in fp_test_dfs:\n",
    "            sheet_name = \"Test_dataset_\"+str(i)\n",
    "            fp_test_df.to_excel(writer, sheet_name=sheet_name,index=False)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8188 2012 1123\n"
     ]
    }
   ],
   "source": [
    "# train_ap1 = pd.DataFrame({\"files\": clean_train_file_names})\n",
    "# validation_ap1 = pd.DataFrame({\"files\": clean_validation_file_names})\n",
    "# test_ap1 = pd.DataFrame({\"files\": clean_test_file_names})\n",
    "# print(len(clean_train_file_names), len(clean_test_file_names), len(clean_validation_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a excel writer object\n",
    "# with pd.ExcelWriter(\"../../data/clean_data/approach1/20240105_clean_data_approach1_method2.xlsx\") as writer:\n",
    "#     # use to_excel function and specify the sheet_name and index \n",
    "#     # to store the dataframe in specified sheet\n",
    "#     train_ap1.to_excel(writer, sheet_name=\"train_dataset\", index=False)\n",
    "#     validation_ap1.to_excel(writer, sheet_name=\"validation_dataset\", index=False)\n",
    "#     test_ap1.to_excel(writer, sheet_name=\"test_dataset\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 68 148\n"
     ]
    }
   ],
   "source": [
    "# dirty_train_ap1 = pd.DataFrame({\"files\": dirty_train_files})\n",
    "# dirty_validation_ap1 = pd.DataFrame({\"files\": dirty_validation_files})\n",
    "# dirty_test_ap1 = pd.DataFrame({\"files\": dirty_test_files})\n",
    "# print(len(dirty_train_files), len(dirty_validation_files), len(dirty_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a excel writer object\n",
    "# with pd.ExcelWriter(\"../../data/dirty_data/20240105_dirty_data_approach1_method2.xlsx\") as writer:\n",
    "#     # use to_excel function and specify the sheet_name and index \n",
    "#     # to store the dataframe in specified sheet\n",
    "#     dirty_train_ap1.to_excel(writer, sheet_name=\"train_dataset\", index=False)\n",
    "#     dirty_validation_ap1.to_excel(writer, sheet_name=\"validation_dataset\", index=False)\n",
    "#     dirty_test_ap1.to_excel(writer, sheet_name=\"test_dataset\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
