{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from data_repository import DataRepository\n",
    "from confident_learning import MyConfidentLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repo = DataRepository(\"../.env\")\n",
    "# Load unclean current data\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = data_repo.load_current_data(clean_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names, validation_file_names, test_file_names = data_repo.load_unclean_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8649, 9000) (8649,) (1191, 9000) (1191,)\n",
      "8649 1191\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_validation.shape, y_validation.shape)\n",
    "print(len(train_file_names), len(validation_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 1718,\n",
       " 'normal': 1734,\n",
       " 'overcurrent': 1709,\n",
       " 'overheating': 1766,\n",
       " 'zero': 1722}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_repo.count_labels(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Label encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi danh sách labels thành mã số\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_validation = label_encoder.transform(y_validation)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['error', 'normal', 'overcurrent', 'overheating', 'zero']\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(list(label_encoder.classes_))\n",
    "print(label_encoder.transform(list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we have to check the model's accuracy on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model's underfit, the model won't fit the data, therefore the out-of-sample probality won't describe correctly the data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True)\n",
    "train_acc = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5, verbose=10)\n",
    "validation_acc = cross_val_score(model, X_validation, y_validation, scoring='accuracy', cv=5, verbose=10)\n",
    "test_acc = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092418685664807 0.8958333333333333 0.9109841426110193\n"
     ]
    }
   ],
   "source": [
    "print(np.average(train_acc), np.average(test_acc), np.average(validation_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the accuracy of the cross validation are reasonable, confident learning can be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting out of sample predicted probality__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Getting out of sample probality\n",
      "[-] Finished getting out of sample probality with shape: (8649, 5)\n",
      "[+] Getting out of sample probality\n",
      "[-] Finished getting out of sample probality with shape: (1191, 5)\n",
      "[+] Getting out of sample probality\n",
      "[-] Finished getting out of sample probality with shape: (2160, 5)\n"
     ]
    }
   ],
   "source": [
    "train_cl = MyConfidentLearning(X=X_train, y=y_train)\n",
    "train_pred_probs = train_cl.get_out_of_sample_proba()\n",
    "\n",
    "validation_cl = MyConfidentLearning(X=X_validation, y=y_validation)\n",
    "validation_pred_probs = validation_cl.get_out_of_sample_proba()\n",
    "\n",
    "test_cl = MyConfidentLearning(X=X_test, y=y_test)\n",
    "test_pred_probs = test_cl.get_out_of_sample_proba()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing class thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.8616693  0.91322388 0.95346248 0.89964409 0.90142361]\n",
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.91641633 0.90305904 0.92666287 0.83581653 0.91424131]\n",
      "[+] Computing thresholds\n",
      "[-] Finished compute thresholds: [0.84152774 0.92196723 0.92664007 0.82998395 0.90748653]\n"
     ]
    }
   ],
   "source": [
    "# should be a numpy array of length 5\n",
    "train_thresholds = train_cl.compute_class_thresholds()\n",
    "validation_thresholds = validation_cl.compute_class_thresholds()\n",
    "test_thresholds = test_cl.compute_class_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the confident joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0   33   63]\n",
      " [   4 1587   26   92    0]\n",
      " [   0   56 1615    0    0]\n",
      " [  45   33    0 1467    0]\n",
      " [ 109    0    0    0 1483]]\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   2   4]\n",
      " [  0 223   0  18   0]\n",
      " [  0  17 220   0   0]\n",
      " [  7  16   0 173   0]\n",
      " [  4   0   0   0 212]]\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0  16  15]\n",
      " [  0 377   7  18   0]\n",
      " [  0  33 421   0   0]\n",
      " [ 16  21   0 305   0]\n",
      " [ 22   0   0   0 374]]\n"
     ]
    }
   ],
   "source": [
    "C_train = train_cl.compute_confident_joint()\n",
    "C_validation = validation_cl.compute_confident_joint()\n",
    "C_test = test_cl.compute_confident_joint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 - Methods 2: Estimate the labels errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_noise_rate(C, no_of_samples):\n",
    "    num_label_issues = np.sum(C - np.diag(np.diag(C)))\n",
    "    print(f\"Number of label issues: {num_label_issues}\")\n",
    "    print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / no_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 461\n",
      "Estimated noise rate: 5.3%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_train, train_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 148\n",
      "Estimated noise rate: 6.9%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_test, test_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label issues: 68\n",
      "Estimated noise rate: 5.7%\n"
     ]
    }
   ],
   "source": [
    "caculate_noise_rate(C_validation, validation_cl.X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning label issues\n",
    "\n",
    "First indentify the label issues, which are the off-diagonal elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finding labels issue indeces:\n",
      "[-] Finished compute confident joint:\n",
      "Issue indices: 461\n",
      "[+] Finding labels issue indeces:\n",
      "[-] Finished compute confident joint:\n",
      "Issue indices: 68\n",
      "[+] Finding labels issue indeces:\n",
      "[-] Finished compute confident joint:\n",
      "Issue indices: 148\n"
     ]
    }
   ],
   "source": [
    "train_issue_indices = train_cl.find_label_issues()\n",
    "validation_issue_indices = validation_cl.find_label_issues()\n",
    "test_issue_indices = test_cl.find_label_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check the confident joint again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the label errors found by Confident Learning\n",
    "clean_X_train = np.delete(X_train, train_issue_indices, axis=0) \n",
    "clean_y_train = np.delete(y_train, train_issue_indices)\n",
    "clean_train_file_names = np.delete(train_file_names, train_issue_indices)\n",
    "clean_train_pred_probs = np.delete(train_cl.pred_probs, train_issue_indices, axis=0)\n",
    "\n",
    "clean_X_validation = np.delete(X_validation, validation_issue_indices, axis=0) \n",
    "clean_y_validation = np.delete(y_validation, validation_issue_indices)\n",
    "clean_validation_file_names = np.delete(validation_file_names, validation_issue_indices)\n",
    "clean_validation_pred_probs = np.delete(validation_cl.pred_probs, validation_issue_indices, axis=0)\n",
    "\n",
    "clean_X_test = np.delete(X_test, test_issue_indices, axis=0) \n",
    "clean_y_test = np.delete(y_test, test_issue_indices)\n",
    "clean_test_file_names = np.delete(test_file_names, test_issue_indices)\n",
    "clean_test_pred_probs = np.delete(test_cl.pred_probs, test_issue_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint(pred_probs: np.ndarray, thresholds: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    print(\"[+] Computing confident joint\")\n",
    "    n_examples, n_classes = pred_probs.shape\n",
    "    confident_joint = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    for data_idx in range(n_examples):\n",
    "        i = labels[data_idx]    #y_noise\n",
    "        j = None                #y_true -> to find\n",
    "        #Lưu ý điểm mình bị sai: vị trí của chúng không ứng với label\n",
    "        p_j = -1\n",
    "        for candidate_j in range(n_classes):\n",
    "            p = pred_probs[data_idx, candidate_j]\n",
    "            if p >= thresholds[candidate_j] and p > p_j:\n",
    "                j = candidate_j\n",
    "                p_j = p\n",
    "        if j is not None:\n",
    "            confident_joint[i][j] += 1\n",
    "    print(\"[-] Finished compute confident joint:\")\n",
    "    print(confident_joint)\n",
    "    return confident_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check the confident joint again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0    0    0]\n",
      " [   0 1587    0    0    0]\n",
      " [   0    0 1615    0    0]\n",
      " [   0    0    0 1467    0]\n",
      " [   0    0    0    0 1483]]\n",
      "Validation dataset\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   0   0]\n",
      " [  0 223   0   0   0]\n",
      " [  0   0 220   0   0]\n",
      " [  0   0   0 173   0]\n",
      " [  0   0   0   0 212]]\n",
      "Test dataset\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0   0   0]\n",
      " [  0 377   0   0   0]\n",
      " [  0   0 421   0   0]\n",
      " [  0   0   0 305   0]\n",
      " [  0   0   0   0 374]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset:\")\n",
    "clean_C_train = compute_confident_joint(pred_probs=clean_train_pred_probs, thresholds=train_thresholds, labels=clean_y_train)\n",
    "\n",
    "print(\"Validation dataset\")\n",
    "clean_C_validation = compute_confident_joint(pred_probs=clean_validation_pred_probs, thresholds=validation_thresholds, labels=clean_y_validation)\n",
    "\n",
    "print(\"Test dataset\")\n",
    "clean_C_test = compute_confident_joint(pred_probs=clean_test_pred_probs, thresholds=test_thresholds, labels=clean_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The results: all the off-diagonal elements are removed!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if their subtraction are corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8188, 9000) 8188 8188 (8188, 5)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[1302    0    0   33   63]\n",
      " [   4 1587   26   92    0]\n",
      " [   0   56 1615    0    0]\n",
      " [  45   33    0 1467    0]\n",
      " [ 109    0    0    0 1483]]\n",
      "{0: 1718, 1: 1734, 2: 1709, 3: 1766, 4: 1722}\n",
      "{0: 1622, 1: 1612, 2: 1653, 3: 1688, 4: 1613}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_train.shape, len(clean_y_train), len(clean_train_file_names), clean_train_pred_probs.shape)\n",
    "train_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(train_cl.y))\n",
    "print(data_repo.count_labels(clean_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1123, 9000) 1123 1123 (1123, 5)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[193   0   0   2   4]\n",
      " [  0 223   0  18   0]\n",
      " [  0  17 220   0   0]\n",
      " [  7  16   0 173   0]\n",
      " [  4   0   0   0 212]]\n",
      "{0: 237, 1: 247, 2: 237, 3: 223, 4: 247}\n",
      "{0: 231, 1: 229, 2: 220, 3: 200, 4: 243}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_validation.shape, len(clean_y_validation), len(clean_validation_file_names), clean_validation_pred_probs.shape)\n",
    "validation_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(validation_cl.y))\n",
    "print(data_repo.count_labels(clean_y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2012, 9000) 2012 2012 (2012,)\n",
      "[+] Computing confident joint\n",
      "[-] Finished compute confident joint:\n",
      "[[349   0   0  16  15]\n",
      " [  0 377   7  18   0]\n",
      " [  0  33 421   0   0]\n",
      " [ 16  21   0 305   0]\n",
      " [ 22   0   0   0 374]]\n",
      "{0: 445, 1: 419, 2: 454, 3: 411, 4: 431}\n",
      "{0: 414, 1: 394, 2: 421, 3: 374, 4: 409}\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_test.shape, len(clean_X_test), len(clean_X_test), clean_test_file_names.shape)\n",
    "test_cl.compute_confident_joint()\n",
    "print(data_repo.count_labels(test_cl.y))\n",
    "print(data_repo.count_labels(clean_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models with different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 94.0%\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "# Train model on original, possibly noisy data.\n",
    "model.fit(train_cl.X, train_cl.y)\n",
    "# Evaluate model on test split with ground truth labels.\n",
    "preds = model.predict(X_validation)\n",
    "acc_original = accuracy_score(preds, y_validation)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with clean data: 95.5%\n"
     ]
    }
   ],
   "source": [
    "clean_model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "# Train model on original, possibly noisy data.\n",
    "clean_model.fit(clean_X_train, clean_y_train)\n",
    "# Evaluate model on test split with ground truth labels.\n",
    "clean_preds = clean_model.predict(clean_X_validation)\n",
    "new_acc = accuracy_score(clean_preds, clean_y_validation)\n",
    "print(f\"Accuracy with clean data: {round(new_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 95.5%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test split with ground truth labels.\n",
    "preds = model.predict(clean_X_validation)\n",
    "test_acc = accuracy_score(clean_preds, clean_y_validation)\n",
    "print(f\"Accuracy with original data: {round(test_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8188 2012 1123\n"
     ]
    }
   ],
   "source": [
    "train_ap1 = pd.DataFrame({\"files\": clean_train_file_names})\n",
    "validation_ap1 = pd.DataFrame({\"files\": clean_validation_file_names})\n",
    "test_ap1 = pd.DataFrame({\"files\": clean_test_file_names})\n",
    "print(len(clean_train_file_names), len(clean_test_file_names), len(clean_validation_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a excel writer object\n",
    "with pd.ExcelWriter(\"../../data/clean_data/approach1/20240105_clean_data_approach1_method2.xlsx\") as writer:\n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    train_ap1.to_excel(writer, sheet_name=\"train_dataset\", index=False)\n",
    "    validation_ap1.to_excel(writer, sheet_name=\"validation_dataset\", index=False)\n",
    "    test_ap1.to_excel(writer, sheet_name=\"test_dataset\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = np.array(train_file_names)\n",
    "test_file_names = np.array(test_file_names)\n",
    "validation_file_names = np.array(validation_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 68 148\n"
     ]
    }
   ],
   "source": [
    "dirty_train_files = train_file_names[train_issue_indices]\n",
    "dirty_validation_files = validation_file_names[validation_issue_indices]\n",
    "dirty_test_files = test_file_names[test_issue_indices]\n",
    "print(len(dirty_train_files), len(dirty_validation_files), len(dirty_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 68 148\n"
     ]
    }
   ],
   "source": [
    "dirty_train_ap1 = pd.DataFrame({\"files\": dirty_train_files})\n",
    "dirty_validation_ap1 = pd.DataFrame({\"files\": dirty_validation_files})\n",
    "dirty_test_ap1 = pd.DataFrame({\"files\": dirty_test_files})\n",
    "print(len(dirty_train_files), len(dirty_validation_files), len(dirty_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a excel writer object\n",
    "with pd.ExcelWriter(\"../../data/dirty_data/20240105_dirty_data_approach1_method2.xlsx\") as writer:\n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    dirty_train_ap1.to_excel(writer, sheet_name=\"train_dataset\", index=False)\n",
    "    dirty_validation_ap1.to_excel(writer, sheet_name=\"validation_dataset\", index=False)\n",
    "    dirty_test_ap1.to_excel(writer, sheet_name=\"test_dataset\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
